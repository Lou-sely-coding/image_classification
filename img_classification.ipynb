{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-20T10:19:57.816681Z",
     "start_time": "2022-05-20T08:56:43.791598Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The overall accuracy is: 0.744\n",
      "The precision is: 0.7493\n",
      "The recall is: 0.7472\n"
     ]
    }
   ],
   "source": [
    "from skimage.feature import daisy\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow import keras\n",
    "\n",
    "# import the fashion_mnist dataset from keras\n",
    "fashion_mnist = keras.datasets.fashion_mnist\n",
    "\n",
    "(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()\n",
    "\n",
    "# setting the parameters for the daisy feature extractor\n",
    "\n",
    "step = 7\n",
    "radius = 3\n",
    "rings = 2\n",
    "histograms = 6\n",
    "orientations = 8\n",
    "\n",
    "# the length each feature vector will have\n",
    "vec_length = (rings * histograms + 1) * orientations\n",
    "\n",
    "# the number of features per image\n",
    "daisies = int((train_images[0].shape[0] / step) ** 2)\n",
    "\n",
    "def create_features(images): \n",
    "\n",
    "    daisy_feature = np.zeros(shape=(daisies * len(images), vec_length))\n",
    "\n",
    "    # create the 16 features for each image\n",
    "    d = 0\n",
    "    for i in range(len(images)):\n",
    "\n",
    "        # we don't need the image\n",
    "        descs = daisy(images[i], step=step, radius=radius, rings=rings, histograms=histograms,\n",
    "                      orientations=orientations, visualize=False)\n",
    "\n",
    "        # reducing the dimension\n",
    "        daisy_array = descs.reshape(daisies, vec_length)\n",
    "\n",
    "        for j, feature_idx in enumerate(range(d, d + daisies)):\n",
    "            daisy_feature[feature_idx] = daisy_array[j]\n",
    "\n",
    "        d += daisies\n",
    "    \n",
    "    return daisy_feature\n",
    "\n",
    "\n",
    "daisy_train = create_features(train_images)\n",
    "daisy_test = create_features(test_images)\n",
    "\n",
    "#implementing k-means\n",
    "\n",
    "# number of clusters\n",
    "k = 60\n",
    "\n",
    "# randomly initialize the centroids for k means\n",
    "np.random.seed(0)\n",
    "initial_centroids = np.zeros(shape=(k, vec_length))\n",
    "random_indices = random.sample(range(len(daisy_train)), k)\n",
    "\n",
    "for i, rand in enumerate(random_indices):\n",
    "    initial_centroids[i] = daisy_train[rand]\n",
    "\n",
    "\n",
    "# get the euclidean distance between each feature and each centroid\n",
    "def calculate_distance(features, centroids):\n",
    "    distances = (-2 * np.dot(centroids, features.T) + np.sum(features ** 2, axis=1) + np.sum(centroids ** 2, axis=1)[:,\n",
    "                                                                                      np.newaxis]).T\n",
    "    return distances\n",
    "\n",
    "\n",
    "# assign each feature to closest centroid\n",
    "\n",
    "def centroid_assign_features(features, distances, k):\n",
    "    # the dictionary stores as the centroids as key and the assigned features as values\n",
    "    visual_dictionary = {c: [] for c in range(k)}\n",
    "\n",
    "    for i, dis in enumerate(distances):\n",
    "        # find the index of the closest cluster for each feature\n",
    "        min_dis_index = np.argmin(dis)\n",
    "        # append the feature to the respective cluster in the dictionary\n",
    "        visual_dictionary[min_dis_index].append(features[i])\n",
    "\n",
    "    return visual_dictionary\n",
    "\n",
    "\n",
    "# calculate the new centroids\n",
    "\n",
    "def calculate_new_centroids(visual_dictionary, vec_length, k):\n",
    "    updated_centroids = np.zeros(shape=(k, vec_length))\n",
    "\n",
    "    for i, features in enumerate(visual_dictionary.values()):\n",
    "        # take the sum of all features for each cluster\n",
    "        stacked_array = np.vstack(features)\n",
    "        # then take the average which will be the updated centroid\n",
    "        new_centroid = np.ndarray.mean(stacked_array, axis=0)\n",
    "        updated_centroids[i] = new_centroid\n",
    "\n",
    "    return updated_centroids\n",
    "\n",
    "\n",
    "def CreateDictionary(initial_centroids, features, vec_length, k):\n",
    "    # the first iteration starts with the randomly created centroids\n",
    "    new_centroids = initial_centroids\n",
    "\n",
    "    for i in range(500):\n",
    "\n",
    "        # calling the above created functions\n",
    "        distances = calculate_distance(features, new_centroids)\n",
    "\n",
    "        visual_dictionary = centroid_assign_features(features, distances, k)\n",
    "\n",
    "        updated_centroids = calculate_new_centroids(visual_dictionary, vec_length, k)\n",
    "\n",
    "        # check if the centroids have changed since the last iteration\n",
    "        if np.array_equal(new_centroids, updated_centroids):\n",
    "            # if there is no change the final centroids have been found\n",
    "            return visual_dictionary, distances, updated_centroids, i\n",
    "\n",
    "        new_centroids = updated_centroids\n",
    "\n",
    "    return visual_dictionary, distances, updated_centroids, i\n",
    "\n",
    "\n",
    "visual_dictionary, all_distances_train, centroids, num_iter = CreateDictionary(initial_centroids, daisy_train,\n",
    "                                                                               vec_length, k)\n",
    "\n",
    "# find the distance of each of the test features to each of the final centroids\n",
    "\n",
    "all_distances_test = calculate_distance(daisy_test, centroids)\n",
    "\n",
    "# get the feature which is closest to each of the centroids\n",
    "closest_index = np.argmin(all_distances_train, axis=0)\n",
    "# find the image of the closest feature as well as the feature cannot be displayed as an image, since it is a vector of orientations\n",
    "closest_image_index = closest_index / daisies\n",
    "closest_image_index = closest_image_index.astype(int)\n",
    "\n",
    "\n",
    "# create the folder in which the closest features will be stored\n",
    "\n",
    "outdir = './cluster_features'\n",
    "\n",
    "if not os.path.exists(outdir):\n",
    "    os.mkdir(outdir)\n",
    "\n",
    "for i, j in enumerate(closest_index):\n",
    "    outname = 'cluster_feature' + str(i + 1) + '.txt'\n",
    "\n",
    "    fullname = os.path.join(outdir, outname)\n",
    "\n",
    "    np.savetxt(fullname, daisy_train[j])\n",
    "\n",
    "# https://stackoverflow.com/questions/47143836/pandas-dataframe-to-csv-raising-ioerror-no-such-file-or-directory\n",
    "\n",
    "# create the folder in which the images of the closest features will be stored\n",
    "outdir = './cluster_images'\n",
    "if not os.path.exists(outdir):\n",
    "    os.mkdir(outdir)\n",
    "\n",
    "for i, j in enumerate(closest_image_index):\n",
    "    outname = 'cluster_image' + str(i + 1) + '.jpeg'\n",
    "\n",
    "    fullname = os.path.join(outdir, outname)\n",
    "\n",
    "    plt.imsave(fullname, train_images[j], cmap=plt.cm.gray)\n",
    "\n",
    "\n",
    "# softmax is used to calculate the probability of an observation belonging to a class\n",
    "# 'softmin' is used here as we need to give a higher probability to a feature belonging to a cluster if the distance is smaller\n",
    "\n",
    "def softmin(x):\n",
    "    # Compute softmin values for each sets of scores in x\n",
    "    e_x = np.exp(np.max(x) - x)\n",
    "    return e_x / e_x.sum(axis=0)\n",
    "\n",
    "\n",
    "# NOTE: Different values for n for the nearest neigbour have been used, however, the best results were achieved\n",
    "# by using all clusters (the probability of a feature belonging to each of the clusters)\n",
    "\n",
    "def ComputeHistogram(train_images, test_images, k, all_distances_train, all_distances_test, daisies):\n",
    "    # create an empty histogram for all images\n",
    "    hist_matrix_train = np.zeros(shape=(len(train_images), k))\n",
    "\n",
    "    # a counter used to loop through each feature of an image\n",
    "    d = 0\n",
    "    for i in range(len(train_images)):\n",
    "        # create an empty histogram for all features of an image\n",
    "        hist = np.zeros(shape=(k))\n",
    "        for j in range(d, d + int(daisies)):\n",
    "            # the probability for each feature to belong to a cluster is calculated by calling softmin\n",
    "            hist += softmin(all_distances_train[j])\n",
    "        # insert the histogram for each image to the histogram array for all images\n",
    "        hist_matrix_train[i] = hist / daisies\n",
    "        d += daisies\n",
    "\n",
    "    # the same steps are conducted for the test images\n",
    "    hist_matrix_test = np.zeros(shape=(len(test_images), k))\n",
    "\n",
    "    d = 0\n",
    "    for i in range(len(test_images)):\n",
    "        hist = np.zeros(shape=(k))\n",
    "        for j in range(d, d + daisies):\n",
    "            hist += softmin(all_distances_test[j])\n",
    "        hist_matrix_test[i] = hist / daisies\n",
    "        d += daisies\n",
    "\n",
    "    return hist_matrix_train, hist_matrix_test\n",
    "\n",
    "\n",
    "hist_matrix_train, hist_matrix_test = ComputeHistogram(train_images, test_images, k, all_distances_train,\n",
    "                                                       all_distances_test, daisies)\n",
    "\n",
    "\n",
    "\n",
    "def histogram_intersection(hist1, hist2, widthHist):\n",
    "    sumVal = 0\n",
    "    for i in range(widthHist):\n",
    "        # get the smaller value for each bin to calculate the amount of intersection\n",
    "        sumVal += min(hist1[i], hist2[i])\n",
    "    return sumVal\n",
    "\n",
    "\n",
    "def MatchHistogram(hist_matrix_train, hist_matrix_test, test_labels, k):\n",
    "    # this saves the indices of the train images which have the highest intersection for each test image\n",
    "    best_match_indices = []\n",
    "\n",
    "    for hist_test in hist_matrix_test:\n",
    "        hist_intersec_score = 0\n",
    "        for j, hist_train in enumerate(hist_matrix_train):\n",
    "            # calculate the intersection for each test image with each train image\n",
    "            score = histogram_intersection(hist_test, hist_train, k)\n",
    "            # find the most overlapping train histogram for each test histogram\n",
    "            if score > hist_intersec_score:\n",
    "                hist_intersec_score = score\n",
    "                # save the index of the train histogram which has the highest intersection\n",
    "                best_match_index = j\n",
    "        best_match_indices.append(best_match_index)\n",
    "\n",
    "    # get the predicted labels by assigning the label of the best matched train image to the test image\n",
    "    pred_labels = np.zeros(shape=(len(test_labels)))\n",
    "\n",
    "    for i in range(len(test_labels)):\n",
    "        pred_labels[i] = train_labels[best_match_indices[i]]\n",
    "\n",
    "    return pred_labels\n",
    "\n",
    "#NOTE: Due to computational constraints only 1000 test images are matched\n",
    "\n",
    "pred_labels = MatchHistogram(hist_matrix_train, hist_matrix_test[0:1000], test_labels[0:1000], k)\n",
    "\n",
    "def CalculatePerformaneMetric(test_labels, pred_labels):\n",
    "    # calculate the true positive, false positive, false negative and true negative rates\n",
    "\n",
    "    TP = np.zeros(shape=10)\n",
    "    FP = np.zeros(shape=10)\n",
    "    FN = np.zeros(shape=10)\n",
    "    TN = np.zeros(shape=10)\n",
    "\n",
    "    for i in range(10):\n",
    "        TP[i] = np.sum((test_labels == i) * (pred_labels == i))\n",
    "        FP[i] = np.sum((test_labels != i) * (pred_labels == i))\n",
    "        FN[i] = np.sum((test_labels == i) * (pred_labels != i))\n",
    "        TN[i] = np.sum((test_labels != i) * (pred_labels != i))\n",
    "\n",
    "    # use the above calculated measures to get accuracy, precision and recall\n",
    "\n",
    "    accuracy_overall = np.around(((TP) / (TP + TN + FP + FN)).sum(), 4)\n",
    "    precision = np.around((TP / (TP + FP)).mean(), 4)\n",
    "    recall = np.around((TP / (TP + FN)).mean(), 4)\n",
    "    class_wise_accuracy = np.around(TP / (TP + FN), 4)\n",
    "\n",
    "    return accuracy_overall, precision, recall, class_wise_accuracy\n",
    "\n",
    "# NOTE: Since we only matched the first 1000 test images, we also will take only the first 1000 test labels\n",
    "accuracy_overall, precision, recall, class_wise_accuracy = CalculatePerformaneMetric(test_labels[0:1000], pred_labels)\n",
    "\n",
    "print('The overall accuracy is:', accuracy_overall)\n",
    "print('The precision is:', precision)\n",
    "print('The recall is:', recall)\n",
    "\n",
    "\n",
    "'''\n",
    "References:\n",
    "\n",
    "desertnaut(2016). Answer on stack overflow to question: How to implement the Softmax function in Python? \n",
    "    Retrieved from: https://stackoverflow.com/questions/34968722/how-to-implement-the-softmax-function-in-python\n",
    "\n",
    "Dey, S.(2016). Distance Matrix Vectorization Trick.\n",
    "    Retrieved from: https://medium.com/@souravdey/l2-distance-matrix-vectorization-trick-26aa3247ac6c\n",
    "\n",
    "Melli, G.(2021). Softmin Activation Function. \n",
    "    Retrieved from: https://www.gabormelli.com/RKB/Softmin_Activation_Function\n",
    "\n",
    "''';"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
